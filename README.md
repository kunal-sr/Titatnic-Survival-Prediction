## ğŸš¢ Titanic - Machine Learning from Disaster

This repository contains my solution for the Titanic - Machine Learning from Disaster competition on Kaggle.

The goal of this competition is to predict which passengers survived the Titanic shipwreck using machine learning techniques.

---

# ğŸ”— Competition Link:
**[Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)**

---

```

ğŸ“‚ Repository Structure
â”œâ”€â”€ train.csv              # Training dataset
â”œâ”€â”€ test.csv               # Test dataset
â”œâ”€â”€ gender_submission.csv  # Kaggle baseline submission
â”œâ”€â”€ main.py                # Model training & prediction script
â”œâ”€â”€ submission.csv         # My final Kaggle submission
â””â”€â”€ README.md
```

---

# ğŸ§  Approach

1ï¸âƒ£ Data Preprocessing

- Combined train and test datasets for consistent preprocessing
- Extracted Title from passenger names
- Grouped rare titles into a single category (Rare)
- Standardized similar titles (e.g., Mlle â†’ Miss, Mme â†’ Mrs)

2ï¸âƒ£ Missing Value Handling

- Age filled using median age grouped by passenger Title (smarter imputation)
- Fare filled using median
- Embarked filled using mode

3ï¸âƒ£ Feature Engineering

- Created FamilySize feature:
- FamilySize = SibSp + Parch + 1

---

# Created IsAlone feature:

```
IsAlone = 1 if FamilySize == 1 else 0
```

--- 

# One-hot encoding for:

- Title
- Embarked

---

# Converted:

Sex â†’ numerical (male=0, female=1)

---

# Dropped unused columns:

- Name
- Ticket
- Cabin

  ---

# ğŸ¤– Model Used

- Random Forest Classifier


```
RandomForestClassifier(
    n_estimators=300,
    max_depth=7,
    random_state=42
)
```

- Why Random Forest?
- Handles non-linearity well
- Robust to overfitting (with depth control)
- Performs strongly on structured/tabular data

  ---

# ğŸ Submission

Predictions were generated on the processed test set and saved to:

```
submission.csv
```

Format:

```
PassengerId,Survived
892,0
893,1
...
```

---

# ğŸ“Š Results

Model: Random Forest

- Engineered features significantly improved prediction quality
- Title-based age imputation improved model stability
- (You can add your Kaggle score here if you'd like.)

  ---

# ğŸš€ How to Run

Clone the repository:

```
git clone https://github.com/yourusername/titanic-ml.git
```

Install dependencies:

```
pip install pandas numpy scikit-learn matplotlib
```

Run the script:

```
python main.py
```
---

The submission file will be generated automatically.

# ğŸ“Œ Key Learnings

- Feature engineering matters more than model complexity.
- Smart missing value handling improves performance.
- Random Forest is a powerful baseline for tabular ML problems.

Kaggle competitions are great for practical ML experience.
